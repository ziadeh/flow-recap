/**\n * Speaker Diarization Example\n * \n * Demonstrates how to use the comprehensive speaker diarization system\n * to identify and segment speakers in audio recordings.\n */\n\nimport { speakerDiarizationService } from '../electron/services'\nimport * as path from 'path'\n\n/**\n * Example 1: Basic speaker diarization\n */\nasync function basicDiarization() {\n  console.log('\\n=== Example 1: Basic Diarization ===')\n  \n  const audioPath = path.join(__dirname, 'sample-audio.wav')\n  \n  // Check availability\n  const availability = await speakerDiarizationService.isAvailable()\n  if (!availability.available) {\n    console.error('Diarization not available:', availability.error)\n    return\n  }\n  \n  console.log('✓ Diarization available')\n  console.log('  Neural pipeline:', availability.hasNeuralPipeline)\n  console.log('  SpeechBrain:', availability.hasSpeechBrain)\n  \n  // Perform diarization\n  const result = await speakerDiarizationService.diarize(audioPath)\n  \n  if (!result.success) {\n    console.error('Diarization failed:', result.error)\n    return\n  }\n  \n  console.log(`\\n✓ Detected ${result.numSpeakers} speakers`)\n  console.log(`  Total segments: ${result.segments.length}`)\n  console.log(`  Audio duration: ${result.audioDuration.toFixed(1)}s`)\n  \n  // Display segments\n  console.log('\\nSegments:')\n  result.segments.slice(0, 5).forEach(seg => {\n    console.log(\n      `  [${seg.start.toFixed(2)}s - ${seg.end.toFixed(2)}s] ` +\n      `${seg.speaker} (${(seg.confidence * 100).toFixed(0)}%)`\n    )\n  })\n  if (result.segments.length > 5) {\n    console.log(`  ... and ${result.segments.length - 5} more`)\n  }\n}\n\n/**\n * Example 2: Diarization with custom configuration\n */\nasync function customConfigDiarization() {\n  console.log('\\n=== Example 2: Custom Configuration ===')\n  \n  const audioPath = path.join(__dirname, 'sample-audio.wav')\n  \n  const result = await speakerDiarizationService.diarize(audioPath, {\n    numSpeakers: 3,                    // Exact number of speakers\n    similarityThreshold: 0.75,         // Higher threshold = more conservative\n    clusteringMethod: 'agglomerative', // Clustering algorithm\n    device: 'auto',                    // Use GPU if available\n    preprocess: true,                  // Apply preprocessing\n    noiseReduction: true,              // Apply noise reduction\n    detectOverlaps: true,              // Detect overlapping speech\n    segmentDuration: 2.0,              // 2-second segments\n    hopDuration: 0.5                   // 0.5-second hop\n  })\n  \n  if (!result.success) {\n    console.error('Diarization failed:', result.error)\n    return\n  }\n  \n  console.log(`✓ Diarization complete with custom config`)\n  console.log(`  Speakers: ${result.speakers.join(', ')}`)\n  console.log(`  Quality score: ${(result.qualityMetrics.overallConfidence * 100).toFixed(0)}%`)\n}\n\n/**\n * Example 3: Diarization with progress tracking\n */\nasync function diarizationWithProgress() {\n  console.log('\\n=== Example 3: Progress Tracking ===')\n  \n  const audioPath = path.join(__dirname, 'sample-audio.wav')\n  \n  const result = await speakerDiarizationService.diarize(\n    audioPath,\n    { numSpeakers: 2 },\n    (progress) => {\n      console.log(\n        `  [${progress.phase.padEnd(20)}] ${progress.progress.toFixed(0).padStart(3)}% - ${progress.message}`\n      )\n    }\n  )\n  \n  if (result.success) {\n    console.log('✓ Diarization complete')\n  }\n}\n\n/**\n * Example 4: Analyze speaker statistics\n */\nasync function speakerStatistics() {\n  console.log('\\n=== Example 4: Speaker Statistics ===')\n  \n  const audioPath = path.join(__dirname, 'sample-audio.wav')\n  const result = await speakerDiarizationService.diarize(audioPath)\n  \n  if (!result.success) {\n    console.error('Diarization failed:', result.error)\n    return\n  }\n  \n  console.log('\\nSpeaker Statistics:')\n  Object.entries(result.speakerStats).forEach(([speaker, stats]) => {\n    console.log(`\\n${speaker}:`)\n    console.log(`  Total speaking time: ${stats.totalDuration.toFixed(1)}s`)\n    console.log(`  Percentage of speech: ${stats.percentage.toFixed(1)}%`)\n    console.log(`  Number of segments: ${stats.segmentCount}`)\n    console.log(`  Average segment: ${stats.averageSegmentDuration.toFixed(1)}s`)\n    console.log(`  First appears at: ${stats.firstAppearance.toFixed(1)}s`)\n    console.log(`  Last appears at: ${stats.lastAppearance.toFixed(1)}s`)\n  })\n  \n  console.log('\\nQuality Metrics:')\n  const qm = result.qualityMetrics\n  console.log(`  Overall confidence: ${(qm.overallConfidence * 100).toFixed(0)}%`)\n  console.log(`  Speaker clarity: ${(qm.speakerClarityScore * 100).toFixed(0)}%`)\n  console.log(`  Boundary precision: ${(qm.boundaryPrecision * 100).toFixed(0)}%`)\n  console.log(`  Overlap ratio: ${(qm.overlapRatio * 100).toFixed(1)}%`)\n  console.log(`  Silence ratio: ${(qm.silenceRatio * 100).toFixed(1)}%`)\n  console.log(`  Processing time: ${qm.processingTimeSeconds.toFixed(1)}s`)\n  console.log(`  Segments per minute: ${qm.segmentsPerMinute.toFixed(1)}`)\n}\n\n/**\n * Example 5: Detect overlapping speech\n */\nasync function overlappingSpeech() {\n  console.log('\\n=== Example 5: Overlapping Speech Detection ===')\n  \n  const audioPath = path.join(__dirname, 'sample-audio.wav')\n  const result = await speakerDiarizationService.diarize(audioPath, {\n    detectOverlaps: true\n  })\n  \n  if (!result.success) {\n    console.error('Diarization failed:', result.error)\n    return\n  }\n  \n  const overlappingSegments = result.segments.filter(s => s.isOverlapping)\n  \n  console.log(`Found ${overlappingSegments.length} overlapping segments:`)\n  overlappingSegments.forEach(seg => {\n    console.log(\n      `  [${seg.start.toFixed(2)}s - ${seg.end.toFixed(2)}s] ` +\n      `${seg.overlappingSpeakers?.join(' + ')}`\n    )\n  })\n}\n\n/**\n * Example 6: Integrate with transcription\n */\nasync function integrateWithTranscription() {\n  console.log('\\n=== Example 6: Integration with Transcription ===')\n  \n  const audioPath = path.join(__dirname, 'sample-audio.wav')\n  \n  // Step 1: Diarize the audio\n  console.log('Step 1: Diarizing audio...')\n  const diarization = await speakerDiarizationService.diarize(audioPath)\n  \n  if (!diarization.success) {\n    console.error('Diarization failed:', diarization.error)\n    return\n  }\n  \n  // Step 2: Mock transcription (in real app, use actual transcription service)\n  const transcription = [\n    { start: 0.5, end: 3.2, text: \"Hello, how are you today?\" },\n    { start: 3.5, end: 6.0, text: \"I'm doing great, thanks for asking!\" },\n    { start: 6.2, end: 9.5, text: \"That's wonderful to hear.\" }\n  ]\n  \n  // Step 3: Assign speakers to transcription\n  console.log('Step 2: Assigning speakers to transcript...')\n  const transcriptWithSpeakers = speakerDiarizationService.assignSpeakersToTranscripts(\n    transcription,\n    diarization.segments\n  )\n  \n  // Step 4: Display results\n  console.log('\\nTranscript with speakers:')\n  transcriptWithSpeakers.forEach(segment => {\n    const speakerLabel = segment.speaker || 'Unknown'\n    const confidence = segment.speakerConfidence\n      ? ` (${(segment.speakerConfidence * 100).toFixed(0)}%)`\n      : ''\n    console.log(`${speakerLabel}${confidence}: ${segment.text}`)\n  })\n}\n\n/**\n * Example 7: Format output as timestamped text\n */\nasync function formattedOutput() {\n  console.log('\\n=== Example 7: Formatted Output ===')\n  \n  const audioPath = path.join(__dirname, 'sample-audio.wav')\n  const result = await speakerDiarizationService.diarize(audioPath)\n  \n  if (!result.success) {\n    console.error('Diarization failed:', result.error)\n    return\n  }\n  \n  // Format as timestamped text\n  const formattedOutput = speakerDiarizationService.formatTimestampedOutput(result)\n  \n  console.log('\\nTimestamped speaker segments:')\n  console.log(formattedOutput)\n}\n\n/**\n * Example 8: Real-time streaming diarization\n */\nasync function streamingDiarization() {\n  console.log('\\n=== Example 8: Streaming Diarization ===')\n  \n  // Start streaming session\n  console.log('Starting streaming session...')\n  const startResult = await speakerDiarizationService.startStreamingSession({\n    sampleRate: 16000,\n    segmentDuration: 2.0,\n    hopDuration: 0.5,\n    similarityThreshold: 0.7,\n    maxSpeakers: 10,\n    device: 'auto'\n  })\n  \n  if (!startResult.success) {\n    console.error('Failed to start streaming:', startResult.error)\n    return\n  }\n  \n  console.log('✓ Streaming session started')\n  \n  // Listen for speaker segments\n  const unsubscribe = speakerDiarizationService.onStreamingSegment((segment) => {\n    console.log(\n      `  [${segment.start.toFixed(2)}s] ${segment.speaker} ` +\n      `(${(segment.confidence * 100).toFixed(0)}%)`\n    )\n  })\n  \n  // Simulate sending audio chunks\n  console.log('Sending audio chunks...')\n  // In real app: send actual audio buffers\n  // for (const chunk of audioChunks) {\n  //   speakerDiarizationService.sendStreamingAudioChunk(chunk)\n  // }\n  \n  // Stop session after a delay\n  setTimeout(async () => {\n    console.log('\\nStopping streaming session...')\n    const segments = await speakerDiarizationService.stopStreamingSession()\n    console.log(`✓ Session ended. Total segments: ${segments.length}`)\n    unsubscribe()\n  }, 5000)\n}\n\n/**\n * Example 9: Handle edge cases\n */\nasync function edgeCases() {\n  console.log('\\n=== Example 9: Edge Cases ===')\n  \n  // Single speaker audio\n  console.log('\\n1. Single speaker audio:')\n  const singleSpeakerPath = path.join(__dirname, 'single-speaker.wav')\n  const result1 = await speakerDiarizationService.diarize(singleSpeakerPath)\n  if (result1.success) {\n    console.log(`  Detected ${result1.numSpeakers} speaker (expected 1)`) \n  }\n  \n  // Very short audio\n  console.log('\\n2. Very short audio (<5 seconds):')\n  const shortAudioPath = path.join(__dirname, 'short-audio.wav')\n  const result2 = await speakerDiarizationService.diarize(shortAudioPath)\n  if (result2.success) {\n    console.log(`  Duration: ${result2.audioDuration.toFixed(1)}s`)\n    console.log(`  Segments: ${result2.segments.length}`)\n  }\n  \n  // Many speakers\n  console.log('\\n3. Many speakers (5+):')\n  const manySpeakersPath = path.join(__dirname, 'conference-call.wav')\n  const result3 = await speakerDiarizationService.diarize(manySpeakersPath, {\n    minSpeakers: 2,\n    maxSpeakers: 15\n  })\n  if (result3.success) {\n    console.log(`  Detected ${result3.numSpeakers} speakers`)\n    console.log(`  Speakers: ${result3.speakers.join(', ')}`)\n  }\n}\n\n/**\n * Example 10: Compare clustering methods\n */\nasync function compareClusteringMethods() {\n  console.log('\\n=== Example 10: Clustering Method Comparison ===')\n  \n  const audioPath = path.join(__dirname, 'sample-audio.wav')\n  const methods: Array<'agglomerative' | 'spectral' | 'online'> = [\n    'agglomerative',\n    'spectral',\n    'online'\n  ]\n  \n  for (const method of methods) {\n    console.log(`\\nTesting ${method} clustering...`)\n    const startTime = Date.now()\n    \n    const result = await speakerDiarizationService.diarize(audioPath, {\n      clusteringMethod: method\n    })\n    \n    const duration = Date.now() - startTime\n    \n    if (result.success) {\n      console.log(`  Speakers detected: ${result.numSpeakers}`)\n      console.log(`  Segments: ${result.segments.length}`)\n      console.log(`  Confidence: ${(result.qualityMetrics.overallConfidence * 100).toFixed(0)}%`)\n      console.log(`  Processing time: ${(duration / 1000).toFixed(1)}s`)\n    }\n  }\n}\n\n// Run examples\nasync function main() {\n  console.log('='.repeat(60))\n  console.log('Speaker Diarization System - Examples')\n  console.log('='.repeat(60))\n  \n  try {\n    // Run examples (comment out ones you don't want to run)\n    await basicDiarization()\n    // await customConfigDiarization()\n    // await diarizationWithProgress()\n    // await speakerStatistics()\n    // await overlappingSpeech()\n    // await integrateWithTranscription()\n    // await formattedOutput()\n    // await streamingDiarization()\n    // await edgeCases()\n    // await compareClusteringMethods()\n    \n    console.log('\\n' + '='.repeat(60))\n    console.log('Examples completed!')\n    console.log('='.repeat(60))\n  } catch (error) {\n    console.error('Error running examples:', error)\n  }\n}\n\n// Run if called directly\nif (require.main === module) {\n  main().catch(console.error)\n}\n\nexport {\n  basicDiarization,\n  customConfigDiarization,\n  diarizationWithProgress,\n  speakerStatistics,\n  overlappingSpeech,\n  integrateWithTranscription,\n  formattedOutput,\n  streamingDiarization,\n  edgeCases,\n  compareClusteringMethods\n}\n